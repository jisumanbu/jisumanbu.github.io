<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[怎样才能生活得更好-读奇特的一生]]></title>
    <url>%2F%E6%80%8E%E6%A0%B7%E6%89%8D%E8%83%BD%E7%94%9F%E6%B4%BB%E5%BE%97%E6%9B%B4%E5%A5%BD-%E5%A5%87%E7%89%B9%E7%9A%84%E4%B8%80%E7%94%9F%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0.html</url>
    <content type="text"><![CDATA[《奇特的一生》以个人传记的方式讲述了柳比歇夫的时间管理法。按照作者的逻辑去读这本书的话，很难抓住重点，正如作者自己所言”作者仍然不能做出最后的结论，给读者出点什么主意”。比如，全书的中心是”怎样才能生活的更好”，但是全文又都是在写柳比歇夫的”时间统计法”，他们之间的逻辑是什么样的？我们有必要或如何将时间统计法应用到我们的生活中？ “时间统计法”本质上是时间管理的一种方式和工具，它服务于目标管理，帮助我们更好的完成我们的人生目标。下面本文就将以”目标驱动”的逻辑来重新梳理《奇特的一生》，弄明白”怎样才能生活的更好”。 定义”过得好” - 明确终生目标主人公柳比歇夫，清楚自己活着的目的，并终生为自己的目标奋斗。最终让他有众多出众的魅力并活出了个&quot;令人惊叹的一生&quot;。 目标明确，定义你最想要的生活 28岁的柳比歇夫就设定了一生的奋斗目标 – 创立生物自然分类法 仿佛有什么崇高的目的，甚至可能领悟了存在的意义。 在通往目标的过程中，他获得了魅力 目标明确，作为榜样，教人应当怎样生活、怎样思考 善于怀疑 不管干什么工作，只要干起来便会感到愉快 不落俗套、独行其是 在通往目标的过程中，他获得了魅力令人惊叹的成就 一生多产，遗产众多 即使到了老年，工作精力和思维效率始终有赠无减 不要害怕目标设定目标是对人生的桎梏么？ 一生始终忠于青年时代的选择，忠于自己的爱好和理想，自认为幸福 再外人眼里，目标明确的生活，令人羡慕目标应是可达的 如果目标不可达，嘛呢目标明确就变成了漫无目的 “最好不是去震惊世界，而是生活在世界上” 怎样实现目标想方设法利用每一分钟，这需要”时间统计”因为&quot;时间是进行创造（实现目标）的条件&quot;，我们必须想方设法利用每一分钟。 利用”时间下脚料” 上下班学英语；跑步听语音读物扩展知识面…… 找出那些”时间下脚料”，需要我们很了解自己的时间 挖掘时间潜力 正确利用工作时间，从时间中找时间 需要我们很了解自己的时间 “计划” 计划就是挑选时间，规定时间，使一切都各得其所 但是计划的制定离不开计算和经验 总结&amp;自我评价 以总结为镜，检查自己的工作效率 检查是否偏离目标 “把一切才能集中到一个目标上，可以取得多得多的成就” 总结的价值和意义：“关心自我” 自我公正的评价，自我奖励与惩罚，满足”功名心” “分门别类” - 知识管理柳比歇夫对各种材料的”分门别类”，其实就是对知识的管理，提高创造力和搭建知识体系。 整理和组织材料这类“事务性的”甚至是“技术性”的工作，有助于创造性的工作 有助于洞察世界 有助于建立一个一个“知识体系” 其实，整理和组织材料的过程本身，就是一种享受“热爱自己的事业” - 事业和目标结合 从事自己喜欢的事业，最好和终生目标契合 个人目标和公司目标的结合 辅助我们实现目标的工具有没有什么工具可以帮助我们，找到&quot;时间的下脚料&quot;；挖掘时间的潜力；更好的做计划…… 有，GTD、&quot;时间统计法&quot;等。 笔者第一次了解&quot;时间统计法&quot;是两年前读德鲁克的《管理的实践》，当时甚是激动与崇拜，并立即实验了起来。如今，我甚至已经记不起来当时用的是哪款APP…… 我想说的是，柳比歇夫的&quot;时间统计法&quot;这一工具真的不一定能适合你我，但是我们关注的重点是&quot;怎样才能生活的更好&quot;，工具不合适，可以再找，不是么？ 柳比歇夫的”时间统计法” 本质是：“事件·时间日志” + 总结 针对过程的“事件·时间日志” 坚持不懈，一天不间断 月度、年度总结 作用 保证高效率 保证旺盛的生命力 学会计算时间，形成经验，服务于制定计划 帮助人把一切才能集中于一个目标上 GTD略 本文导图]]></content>
      <categories>
        <category>目标驱动</category>
      </categories>
      <tags>
        <tag>目标驱动</tag>
        <tag>时间管理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[知识体系总览]]></title>
    <url>%2Fknowledge-hierarchy.html</url>
    <content type="text"><![CDATA[]]></content>
      <categories>
        <category>知识体系</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[RocketMq实战]]></title>
    <url>%2FrocketMq-in-action.html</url>
    <content type="text"><![CDATA[将RocketMQ的producer和listner封装成sender和consumer，力求简单实用。 源码：https://github.com/jisumanbu/RocketMqInAction.git 发送消息封装类为：MqMessageSender，支持以下多种发送方式及异常处理 同步和异步发送消息 发送有序消息 延迟发送消息 异常处理 失败重试 接收并消费消息分为ConcurrentlyConsumer和OrderlyConsumer。 有序消息 -&gt; 继承OrderlyConsumer 非有序消息 -&gt; 可认为支持多线程, 继承ConcurrentlyConsumer自动过滤重复消息，避免重复消费问题 具体实现：AbstractConsumer.isAlreadyConsumed(…)方法。 利用redisson的RSetCache类的add特性来判断消息是否已经消费过了 true if value has been added. false if value already been in collection. 1isAlreadyConsumed = !FedisClient.getClient().getSetCache(hashKeyOfRedis).add(messageKey, 1L, TimeUnit.DAYS); 注意：FedisClient为未实现单例，用户得自行修复以上代码。]]></content>
      <categories>
        <category>MQ</category>
        <category>RocketMQ</category>
      </categories>
      <tags>
        <tag>MQ</tag>
        <tag>RocketMQ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[以企业的方式经营家庭]]></title>
    <url>%2Foperate-family-like-a-company.html</url>
    <content type="text"><![CDATA[想法的产生16年看着《管理的实践》却不知道何为公司；认识老婆后便萌生了以企业的方式经营家庭的想法。想着”以企业的方式经营家庭”可以 促使我去了解”企业”到底是什么，为以后的创业做铺垫 目标管理的实践：家庭目标 &amp; 个人目标的平衡 从一个人变成了一家人，从而必须以组织的角度重新审视”家” 摆脱浑浑噩噩 最近又重新饱受焦虑的困扰，所以迫使着重新梳理了下目标；再加上和老婆生活上的小摩擦让我坚定了这一想法，决定立即实施。 家庭企业的建立家庭适用有限合伙企业从下图中可以很快发现，&quot;有限合伙企业&quot;和&quot;家庭&quot;最贴切（夫妻双方为普通合伙人，父母孩子皆为有限合伙人）。 有限合伙企业成员的角色、权利、义务 夫妻，普通合伙人，以预算的方式分配企业所得，承担无限连带责任 父母，有限合伙人，参与固定分红以及分享一定应急资金，承担有限责任（带孩子等） 孩子，战略性投资项目（因为没有投入和不承担责任所以不应作为合伙人而存在），享受预算优先权，不承担任何责任 合伙协议，按企业法应包含如下内容： 普通合伙人和有限合伙人的姓名或者名称、住所； 略 执行事务合伙人应具备的条件和选择程序； 夫妻两人皆为执行事务合伙人，投票权完全相同 执行事务合伙人权限与违约处理办法； 没有特别附加权限，等同普通合伙人权益； 违约？ 什么约？ 执行事务合伙人的除名条件和更换程序； 不会更换，除非散货 有限合伙人入伙、退伙的条件、程序以及相关责任； 参照婚姻法中离婚的相关条款 有限合伙人和普通合伙人相互转变程序。 合伙人角色不会发生变化 如何经营家庭有限合伙企业目标驱动以目标驱动企业日常运营 目标设定 企业目标和个人目标的平衡 1.2. 根据目标推导出的个人计划 预算式财政（待详细） 保险 风险资金 房租 老婆护肤品 老婆衣服 生活费 旅游]]></content>
      <categories>
        <category>BetterLife</category>
      </categories>
      <tags>
        <tag>strategy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HashMap的工作原理【译文】]]></title>
    <url>%2Fhow-does-a-hashmap-work-in-java.html</url>
    <content type="text"><![CDATA[翻译自How does a HashMap work in JAVA - Coding Geek.欢迎指正错误。 绝大多数JAVA开发者无时无刻不在使用Maps，特别是HashMaps。HashMap提供了即简单又强大的存储和获取数据的方式。但是又有多少人了解Hashmap的内部运行机制呢？最近一段时间，为了深入了解HashMap这一重要的数据结构我阅读了绝大部分java.util.HashMap的源码， 先是基于java 7然后java 8。在这篇博文里，我将为大家展示HashMap的实现原理，在java 8中的新特性在使用中要注意的关于性能和内存的问题，以及一些常见问题。 Contents 内部存储 自适应大小 线程安全 Key的不可变性 JAVA 8中的提升 内存负载 JAVA 7 JAVA 87 性能问题 Skewed HashMap vs well balanced HashMap 自适应大小带来的开销8 总结 Internal storageHashMap的接口是Map&lt;K,V&gt;，这个接口的主要方法有： V put(K key, V value) V get(Object key) V remove(Object key) Boolean containsKey(Object key) HashMap真正存储数据的地方是其内部类：Entry&lt;K, V&gt;, 就是一个简单的键值对外加两个property： 指向另一个Entry的引用，从而使HashMap可以像LinkedList一样存储多个实体 key的hash值，从而避免每次使用时都要重复计算其hash值 下面是Entry在JAVA 7中的部分代码：1234567static class Entry&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; &#123; final K key; V value; Entry&lt;K,V&gt; next; int hash;…&#125; HashMap将数据存储在多个链表中（也称为分桶或分箱），所有的链表又都保存在一个Entry数组中（数组的默认容量为16）。 The following picture shows the inner storageof a HashMap instance with an array of nullable entries.Each Entry can link to another Entry to form a linked list.下面这张图展示了一个HashMap实例的内部存储结构 - 一个支持null节点的数组。 All the keys with the same hash value are put in the same linked list (bucket). Keys with different hash values can end-up in the same bucket. When a user calls put(K key, V value) or get(Object key), the function computes the index of the bucket in which the Entry should be. Then, the function iterates through the list to look for the Entry that has the same key (using the equals() function of the key). In the case of the get(), the function returns the value associated with the entry (if the entry exists). In the case of the put(K key, V value), if the entry exists the function replaces it with the new value otherwise it creates a new entry (from the key and value in arguments) at the head of the singly linked list. This index of the bucket (linked list) is generated in 3 steps by the map: It first gets the hashcode of the key.It rehashes the hashcode to prevent against a bad hashing function from the key that would put all data in the same index (bucket) of the inner arrayIt takes the rehashed hash hashcode and bit-masks it with the length (minus 1) of the array. This operation assures that the index can’t be greater than the size of the array. You can see it as a very computationally optimized modulo function.Here is the JAVA 7 and 8 source code that deals with the index: // the “rehash” function in JAVA 7 that takes the hashcode of the keystatic int hash(int h) { h ^= (h &gt;&gt;&gt; 20) ^ (h &gt;&gt;&gt; 12); return h ^ (h &gt;&gt;&gt; 7) ^ (h &gt;&gt;&gt; 4);}// the “rehash” function in JAVA 8 that directly takes the keystatic final int hash(Object key) { int h; return (key == null) ? 0 : (h = key.hashCode()) ^ (h &gt;&gt;&gt; 16); }// the function that returns the index from the rehashed hashstatic int indexFor(int h, int length) { return h &amp; (length-1);}In order to work efficiently, the size of the inner array needs to be a power of 2, let’s see why. Imagine the array size is 17, the mask value is going to be 16 (size -1). The binary representation of 16 is 0…010000, so for any hash value H the index generated with the bitwise formula “H AND 16” is going to be either 16 or 0. This means that the array of size 17 will only be used for 2 buckets: the one at index 0 and the one at index 16, not very efficient… But, if you now take a size that is a power of 2 like 16, the bitwise index formula is “H AND 15”. The binary representation of 15 is 0…001111 so the index formula can output values from 0 to 15 and the array of size 16 is fully used. For example: if H = 952 , its binary representation is 0..01110111000, the associated index is 0…01000 = 8if H = 1576 its binary representation is 0..011000101000, the associated index is 0…01000 = 8if H = 12356146, its binary representation is 0..0101111001000101000110010, the associated index is 0…00010 = 2if H = 59843, its binary representation is 0..01110100111000011, the associated index is 0…00011 = 3 This is why the array size is a power of two. This mechanism is transparent for the developer: if he chooses a HashMap with a size of 37, the Map will automatically choose the next power of 2 after 37 (64) for the size of its inner array. Auto resizingAfter getting the index, the function (get, put or remove) visits/iterates the associated linked list to see if there is an existing Entry for the given key. Without modification, this mechanism could lead to performance issues because the function needs to iterate through the entire list to see if the entry exists. Imagine that the size of the inner array is the default value (16) and you need to store 2 millions values. In the best case scenario, each linked list will have a size of 125 000 entries (2/16 millions). So, each get(), remove() and put() will lead to 125 000 iterations/operations. To avoid this case, the HashMap has the ability to increase its inner array in order to keep very short linked lists. When you create a HashMap, you can specify an initial size and a loadFactor with the following constructor: public HashMap(int initialCapacity, float loadFactor)If you don’t specify arguments, the default initialCapacity is 16 and the default loadFactor is 0.75. The initialCapacity represents to the size of the inner array of linked lists. Each time you add a new key/value in your Map with put(…), the function checks if it needs to increase the capacity of the inner array. In order to do that, the map stores 2 data: The size of the map: it represents the number of entries in the HashMap. This value is updated each time an Entry is added or removed.A threshold: it’s equal to (capacity of the inner array) * loadFactor and it is refreshed after each resize of the inner arrayBefore adding the new Entry, put(…) checks if size &gt; threshold and if it the case it recreates a new array with a doubled size. Since the size of the new array has changed, the indexing function (which returns the bitwise operation “hash(key) AND (sizeOfArray-1)”) changes. So, the resizing of the array creates twice more buckets (i.e. linked lists) and redistributes all the existing entries into the buckets (the old ones and the newly created). This aim of this resize operation is to decrease the size of the linked lists so that the time cost of put(), remove() and get() methods stays low. All entries whose keys have the same hash will stay in the same bucket after the resizing. But, 2 entries with different hash keys that were in the same bucket before might not be in the same bucket after the transformation. resizing_of_java_hashmap The picture shows a representation before and after the resizing of the inner array. Before the increase, in order to get Entry E, the map had to iterate through a list of 5 elements. After the resizing, the same get() just iterates through a linked list of 2 elements, the get() is 2 times faster after the resizing ! Note: the HashMap only increases the size of the inner array, it doesn’t provide a way to decrease it. Thread SafetyIf you already know HashMaps, you know that is not threads safe, but why? For example imagine that you have a Writer thread that puts only new data into the Map and a Reader thread that reads data from the Map, why shouldn’t it work? Because during the auto-resizing mechanism, if a thread tries to put or get an object, the map might use the old index value and won’t find the new bucket in which the entry is. The worst case scenario is when 2 threads put a data at the same time and the 2 put() calls resize the Map at the same time. Since both threads modify the linked lists at the same time, the Map might end up with an inner-loop in one of its linked lists. If you tries to get a data in the list with an inner loop, the get() will never end. The HashTable implementation is a thread safe implementation that prevents from this situation. But, since all the CRUD methods are synchronized this implementation is very slow. For example, if thread 1 calls get(key1), thread 2 calls get(key2) and thread 3 calls get(key3), only one thread at a time will be able to get its value whereas the 3 of them could access the data at the same time. A smarter implementation of a thread safe HashMap exists since JAVA 5: the ConcurrentHashMap. Only the buckets are synchronized so multiples threads can get(), remove() or put() data at the same time if it doesn’t imply accessing the same bucket or resizing the inner array. It’s better to use this implementation in a multithreaded application. Key immutabilityWhy Strings and Integers are a good implementation of keys for HashMap? Mostly because they are immutable! If you choose to create your own Key class and don’t make it immutable, you might lose data inside the HashMap. Look at the following use case: You have a key that has an inner value “1”You put an object in the HashMap with this keyThe HashMap generates a hash from the hashcode of the Key (so from “1”)The Map stores this hash in the newly created EntryYou modify the inner value of the key to “2”The hash value of the key is modified but the HashMap doesn’t know it (because the old hash value is stored)You try to get your object with your modified keyThe map computes the new hash of your key (so from “2”) to find in which linked list (bucket) the entry isCase 1: Since you modified your key, the map tries to find the entry in the wrong bucket and doesn’t find it Case 2: Luckily, the modified key generates the same bucket as the old key. The map then iterates through the linked list to find the entry with the same key. But to find the key, the map first compares the hash values and then calls the equals() comparison. Since your modified key doesn’t have the same hash as the old hash value (stored in the entry), the map won’t find the entry in the linked-list.Here is a concrete example in Java. I put 2 key-value pairs in my Map, I modify the first key and then try to get the 2 values. Only the second value is returned from the map, the first value is “lost” in the HashMap: public class MutableKeyTest { public static void main(String[] args) { class MyKey { Integer i; public void setI(Integer i) { this.i = i; } public MyKey(Integer i) { this.i = i; } @Override public int hashCode() { return i; } @Override public boolean equals(Object obj) { if (obj instanceof MyKey) { return i.equals(((MyKey) obj).i); } else return false; } } Map&lt;MyKey, String&gt; myMap = new HashMap&lt;&gt;(); MyKey key1 = new MyKey(1); MyKey key2 = new MyKey(2); myMap.put(key1, &quot;test &quot; + 1); myMap.put(key2, &quot;test &quot; + 2); // modifying key1 key1.setI(3); String test1 = myMap.get(key1); String test2 = myMap.get(key2); System.out.println(&quot;test1= &quot; + test1 + &quot; test2=&quot; + test2); } }The output is: “test1= null test2=test 2”. As expected, the Map wasn’t able to retrieve the string 1 with the modified key 1. JAVA 8 improvementsThe inner representation of the HashMap has changed a lot in JAVA 8. Indeed, the implementation in JAVA 7 takes 1k lines of code whereas the implementation in JAVA 8 takes 2k lines. Most of what I’ve said previously is true except the linked lists of entries. In JAVA8, you still have an array but it now stores Nodes that contains the exact same information as Entries and therefore are also linked lists: Here is a part of the Node implementation in JAVA 8: static class Node&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; { final int hash; final K key; V value; Node&lt;K,V&gt; next;So what’s the big difference with JAVA 7? Well, Nodes can be extended to TreeNodes. A TreeNode is a red-black tree structure that stores really more information so that it can add, delete or get an element in O(log(n)). FYI, here is the exhaustive list of the data stored inside a TreeNode static final class TreeNode&lt;K,V&gt; extends LinkedHashMap.Entry&lt;K,V&gt; { final int hash; // inherited from Node&lt;K,V&gt; final K key; // inherited from Node&lt;K,V&gt; V value; // inherited from Node&lt;K,V&gt; Node&lt;K,V&gt; next; // inherited from Node&lt;K,V&gt; Entry&lt;K,V&gt; before, after;// inherited from LinkedHashMap.Entry&lt;K,V&gt; TreeNode&lt;K,V&gt; parent; TreeNode&lt;K,V&gt; left; TreeNode&lt;K,V&gt; right; TreeNode&lt;K,V&gt; prev; boolean red;Red black trees are self-balancing binary search trees. Their inner mechanisms ensure that their length is always in log(n) despite new adds or removes of nodes. The main advantage to use those trees is in a case where many data are in the same index (bucket) of the inner table, the search in a tree will cost O(log(n)) whereas it would have cost O(n) with a linked list. As you see, the tree takes really more space than the linked list (we’ll speak about it in the next part). By inheritance, the inner table can contain both Node (linked list ) and TreeNode (red-black tree). Oracle decided to use both data structures with the following rules:– If for a given index (bucket) in the inner table there are more than 8 nodes, the linked list is transformed into a red black tree– If for a given index (bucket) in the inner table there are less than 6 nodes, the tree is transformed into a linked list internal_storage_java8_hashmap This picture shows an inner array of a JAVA 8 HashMap with both trees (at bucket 0) and linked lists (at bucket 1,2 and 3). Bucket 0 is a Tree because it has more than 8 nodes. Memory overheadJAVA 7 The use of a HashMap comes at a cost in terms of memory. In JAVA 7, a HashMap wraps key-value pairs in Entries. An entry has: a reference to a next entrya precomputed hash (integer)a reference to the keya reference to the valueMoreover, a JAVA 7 HashMap uses an inner array of Entry. Assuming a JAVA 7 HashMap contains N elements and its inner array has a capacity CAPACITY, the extra memory cost is approximately: sizeOf(integer) N + sizeOf(reference) (3*N+C) Where: the size of an integer depends equals 4 bytesthe size of a reference depends on the JVM/OS/Processor but is often 4 bytes.Which means that the overhead is often 16 N + 4 CAPACITY bytes Reminder: after an auto-resizing of the Map, the CAPACITY of the inner array equals the next power of two after N. Note: Since JAVA 7, the HashMap class has a lazy init. That means that even if you allocate a HashMap, the inner array of entry (that costs 4 * CAPACITY bytes) won’t be allocated in memory until the first use of the put() method. JAVA 8 With the JAVA 8 implementation, it becomes a little bit complicated to get the memory usage because a Node can contain the same data as an Entry or the same data plus 6 references and a Boolean (if it’s a TreeNode). If the all the nodes are only Nodes, the memory consumption of the JAVA 8 HashMap is the same as the JAVA 7 HashMap. If the all the nodes are TreeNodes, the memory consumption of a JAVA 8 HashMap becomes: N sizeOf(integer) + N sizeOf(boolean) + sizeOf(reference) (9N+CAPACITY ) In most standards JVM, it’s equal to 44 N + 4 CAPACITY bytes Performance issuesSkewed HashMap vs well balanced HashMap In the best case scenario, the get() and put() methods have a O(1) cost in time complexity. But, if you don’t take care of the hash function of the key, you might end up with very slow put() and get() calls. The good performance of the put() and get depends on the repartition of the data into the different indexes of the inner array (the buckets). If the hash function of your key is ill-designed, you’ll have a skew repartition (no matter how big the capacity of the inner array is). All the put() and get() that use the biggest linked lists of entry will be slow because they’ll need to iterate the entire lists. In the worst case scenario (if most of the data are in the same buckets), you could end up with a O(n) time complexity.Here is a visual example. The first picture shows a skewed HashMap and the second picture a well balanced one. skewed_java_hashmap In the case of this skewed HashMap the get()/put() operations on the bucket 0 are costly. Getting the Entry K will cost 6 iterations well_balanced_java_hashmapIn the case of this well balanced HashMap, getting the Entry K will cost 3 iterations. Both HashMaps store the same amount of data and have the same inner array size. The only difference is the hash (of the key) function that distributes the entries in the buckets. Here is an extreme example in JAVA where I create a hash function that puts all the data in the same bucket then I add 2 million elements. public class Test { public static void main(String[] args) { class MyKey { Integer i; public MyKey(Integer i){ this.i =i; } @Override public int hashCode() { return 1; } @Override public boolean equals(Object obj) { … } } Date begin = new Date(); Map &lt;MyKey,String&gt; myMap= new HashMap&lt;&gt;(2_500_000,1); for (int i=0;i&lt;2_000_000;i++){ myMap.put( new MyKey(i), &quot;test &quot;+i); } Date end = new Date(); System.out.println(&quot;Duration (ms) &quot;+ (end.getTime()-begin.getTime())); } }On my core i5-2500k @ 3.6Ghz it takes more than 45 minutes with java 8u40 (I stopped the process after 45 minutes). Now, If I run the same code but this time I use the following hash function @Override public int hashCode() { int key = 2097152-1; return key+2097152*i; }it takes 46 seconds, which is way better! This hash function has a better repartition than the previous one so the put() calls are faster. And If I run the same code with the following hash function that provides an even better hash repartition @Overridepublic int hashCode() {return i;}it now takes 2 seconds. I hope you realize how important the hash function is. If a ran the same test on JAVA 7, the results would have been worse for the first and second cases (since the time complexity of put is O(n) in JAVA 7 vs O(log(n)) in JAVA 8) When using a HashMap, you need to find a hash function for your keys that spreads the keys into the most possible buckets. To do so, you need to avoid hash collisions. The String Object is a good key because of it has good hash function. Integers are also good because their hashcode is their own value. Resizing overhead If you need to store a lot of data, you should create your HashMap with an initial capacity close to your expected volume. If you don’t do that, the Map will take the default size of 16 with a factorLoad of 0.75. The 11 first put() will be very fast but the 12th (160.75) will recreate a new inner array (with its associated linked lists/trees) with a new capacity of 32. The 13th to 23th will be fast but the 24th (320.75) will recreate (again) a costly new representation that doubles the size of the inner array. The internal resizing operation will appear at the 48th, 96th,192th, … call of put(). At low volume the full recreation of the inner array is fast but at high volume it can takes seconds to minutes. By initially setting your expected size, you can avoid these costly operations. But there is a drawback: if you set a very high array size like 2^28 whereas you’re only using 2^26 buckets in your array, you will waste a lot of memory (approximately 2^30 bytes in this case). ConclusionFor simple use cases, you don’t need to know how HashMaps work since you won’t see the difference between a O(1) and a O(n) or O(log(n)) operation. But it’s always better to understand the underlaying mecanism of one of the most used data structures. Moreover, for a java developer position it’s a typical interview question. At high volume it becomes important to know how it works and to understand the importance of the hash function of the key. I hope this article helped you to have a deep understanding of the HashMap implementation.]]></content>
      <categories>
        <category>DataStructure</category>
      </categories>
      <tags>
        <tag>DataStructure</tag>
        <tag>HashMap</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HTTP、HTTPS、TOKEN的比较 以及 如何选择]]></title>
    <url>%2FHTTP_HTTPS_Token.html</url>
    <content type="text"><![CDATA[HTTP与HTTPS HTTP的缺点 及 解决方案 通信使用明文(不加密)，内容可能被窃听 加密 不验证通信方的身份，因此有可能遭遇伪装（假的server或client） 证书 （支持双向验证，但通常只验证服务端.）SSL客户端的身份验证是可选的，由SSL服务器决定是否验证SSL客户端的身份 无法证明报文的完整性，所以有可能已遭篡改 基于散列值校验（MD5 / SHA-1）等等的完整性校验方案 HTTPS可以完美解决以上问题 HTTP + 加密 + 认证 + 完整性保护 = HTTPS 请求包可以被截获，但无法解密读取 对于HTTPS不做赘述，下面仅附图一张 既然HTTPS那么安全可靠，那为何所有的网站不一直使用HTTPS ？ HTTPS比HTTP慢2到100倍 通信慢 TCP/IP之上还需要SSL通信 消耗更多的CPU和内存 客户端和服务端都必须加密解密运算， 证书的开销 HTTP + TOKEN：每次请请求都携带经过加密且被签名的token 无状态，可快速验证用户的有效性 加密，防止token被读取篡改 但是第三方可以用截取到的token伪造请求，所以client有被劫持的风险；签名和设置有效期可以解决 GET等非敏感请求 设置token的有效期”per session cookies, and per request” – 防CSRF 对于GET，token的签名包含url，确保token和URL是对应的没被更改的 – 请求顶多被窃听 例如Get请求时，将parameters加入到签名里，从而防止被篡改；但仍然可以被监听. POST请求则直接对请求体做MD5计算，将MD5值加入签名. POST等敏感请求，用HTTPS 最终选择 首选HTTPS，如果不用考虑性能和money的话 认证可随意选择，用户名密码或者简单token HTTP + token，请求体、相应体有被监听的风险 提前约定secret、加密方式等 每次请求都生成唯一的经过加密和签名的token 将md5(requestContent/parameter)作为签名的一部分，避免劫持 上面两点无法防止伪装，token可以被窃取，从而有被劫持.可以将md5(requestContent/parameter)作为签名的一部分，避免劫持，顶多有被监听的风险，因为信息不明感，从而可忽略 HTTP和HTTPS组合使用 非敏感信息使用HTTP + token的认证方式 只有在接触敏感信息时才使用HTTPS，如登录 反推TOKEN的特性 加密，不可被窃听 加密方式 秘钥 对于API，一般不要求登录，所以用户名和密钥要提前约定好. 签名，可以证明请求没有没篡改 有效期设置功能，防止被窃取后伪造请求；如果是浏览器访问也可以解决CSRF “per session cookies, and per request tokens.”， 可以解决CSRF，但不能解决监听问题。 API不存在CSRF问题，但如果token永不过期，截取到token便可以伪造请求了，所以可以为API的token设置一个长点的过期时间 所以建议： API 非敏感数据的API过期设的长一些，以免让client频繁请求token 敏感数据直接升级用HTTPS B/S HTTPS + per request token 案例 去哪儿，HTTP+TOKEN 和 HTTPS 非敏感数据采用 HTTP + TOKEN 1String sign = DigestUtils.md5Hex(DigestUtils.md5Hex(secretKey + appKey) + salt); appKey、secretKey提前约定好 没有有效期，截取sign后可以长期伪装client, 发送重复请求。 没有签名， 伪装client，且可以更改请求 亮哥，HTTP + TOKEN 1String sign = md5(userId + timestamps + secret);//per request timestamps server端记录每次请求的timestamps, 当次有效 没有签名， 可以被劫持，从而更改请求 伪装问题 HTTPS不存在伪装问题 有证书保证server身份，所以无法伪装server 在用户名密码不被泄露的情况下，能建立起SSL的就是真正的用户 HTTP虽然无法彻底解决伪装问题，但是因为token的不可更改不可窃听的属性，伪装被降级成转发监听 由于token是不可读，不可修改的，所以第三方无法伪造只能引用. 又因为加了有效期，所以第三方最多只能转发监听 Client-side/Stateless Sessions (客户端持有的/无状态session) stateless 只保存客户数据 JWT不可被第三方 读取 和 更改 签名 = 不可更改 加密 = 不可读 Cross-Site Request Forgery (CSRF - 跨站请求伪造) 由于同一浏览器进程下 Cookie 可见性，导致用户身份被盗用，完成恶意网站脚本中指定的操作 解决方案 “per session cookies, and per request tokens.” 案例一 客户端每个request都必须携带 userId + timestamps + token token = md5(userId + secret + timestamps**)，确保信息不被更改 timestamps只能使用一次 服务端先校验timestamps是否已经被处理，如果已经处理，拒绝请求 服务端根据收到的userId + timestamps和约定好的secret计算token，然后和收到的token进行比较校验]]></content>
      <categories>
        <category>Web</category>
        <category>Security</category>
      </categories>
      <tags>
        <tag>HTTPS</tag>
        <tag>JWT</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis实战（Redis IN ACTION）]]></title>
    <url>%2FredisInAction.html</url>
    <content type="text"><![CDATA[第1章 初识Redis Redis的优点 比关系型数据库更效率、更易用；比内存数据库有数据结构上的优势 高性能内存数据库，但支持持久化《RDB, AOF》 5种数据结构 《string, List, set, hash, zset 第6种数据结构：pub/sub 简单事务 过期机制 主从复制 -&gt; 扩展读性能，故障转移 第4章 数据安全与性能保障 4.1 持久化选项 快照持久化&lt;SNAPSHOT/RDB、AOF&gt; 从服务器的复制启动过程&lt;第一次同步&gt; SNAPSHOT/RDB &lt;高效恢复数据，但是有停顿，丢数据&gt; BGSAVE fork子进程 -&gt; 停顿 &lt;生成子进程的过程会阻塞父进程&gt; 可关闭自动保存，手动触发 内存不足时 -&gt; 使用虚拟内存 master内存使用率应在50%~65% SAVE 没有子进程，故比BGSAVE快 数据完整性 可承受丢失多长时间的数据？ save 60 1000 丢失了那些数据？ -&gt; process_logs() 保证日志的处理结果和处理进度总是同时被记录到快照文件里面 AOF 不丢数据，无停顿，但恢复慢，日志体积大 主从写更新 同步频率&lt;appendfsync选项&gt; always&lt;每次写都触发同步，每次只写入一个命令&gt; -&gt; 最慢 everysec&lt;每秒触发一次同步&gt; -&gt; 推荐 no&lt;操作系统自动决定同步时机&gt; -&gt; 不推荐 重写/压缩AOF文件 BGREWRITEAOF -&gt; 工作机制同BGSAVE，停顿 何时执行BGREWRITEAOF？ auto-aof-rewrite-percentage auto-aof-rewirte-min-size 4.2 复制(replication) 主从复制 &lt;不支持多主复制&gt; 复制的启动过程 主从链 复制中间层 -&gt; 分担主服务器的复制工作 检查硬盘写入 conn.info()…. 4.3 处理系统故障 验证AOF文件 可修复 验证快照文件 不可修复 恢复前需验证快照文件&lt;SHA1/SHA256&gt; 更换故障主服务器、Redis Sentinel的故障转移 4.4 事务 watch + retry 如果数据被更改，收到watch的通知，然后重试 流水线（pipelining）方式：pipe.multi + pipe.execute 一次性发送多个命令，然后等待所有回复出现&lt;如果执行过程中，接到watchError则失败&gt; 4.5 非事务型流水线 pipe = conn.pipeline(false) 《单纯的把多个命令打包，一起发送》]]></content>
      <categories>
        <category>readNote</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RocketMQ二安装与简单实用]]></title>
    <url>%2FrocketMQ-2-installation-and-simple-usages.html</url>
    <content type="text"><![CDATA[目录部署 1 Master 2 Master 2 Master - 2 Salve async 2 Master - 2 Salve sync 控制台安装发送消息的默认约定消费消息的默认约定并行消息(多线程多Queue)有序消息(单线程单一ueue)延迟消息(单线程单一ueue) 部署下载并解压[rocketmq](http://mirrors.tuna.tsinghua.edu.cn/apache/incubator/rocketmq/4.0.0-incubating/rocketmq-all-4.0.0-incubating-bin-release.zip ) 1 Master 启动nameServer 1nohup sh bin/mqnamesrv &amp; 启动broker 1nohup sh bin/mqbroker -n localhost:9876 -c conf/broker.conf &amp; shutdown 12bin/mqshutdown brokerbin/mqshutdown namesrv 2 MasterTODO 2 Master - 2 Salve asyncTODO 2 Master - 2 Salve syncTODO 控制台安装 控制台使用apache/incubator-rocketmq-externals下的rocketmq-console clone： 1https://github.com/apache/incubator-rocketmq-externals.git 打包 1mvn clean package -Dmaven.test.skip=true 启动 1java -jar rocketmq-console-ng-1.0.0.jar --server.port=12581 --rocketmq.config.namesrvAddr=localhost:9876 访问控制台：http://localhost:12581 发送消息的默认约定： 一个JVM一个MqMessageSender实例，后续视情况是否改为prototype 默认发送方式为同步 - sendMsg(String, String, String, String) client没有必要retry rocketMQ已经有retry, MqMessageSender里也会做必要的失败记录和服务不可用时的retry rocketMQ的retry 12defaultSendingTimeout = 3000 millsretryTimesWhenSendFailed = 2 如果发送失败，确保消息不丢失 MqMessageSender会保存消息到DB，同时抛出unCheckedException （TODO）服务可用后，可以从DB中重新发送 应用程序接收到Exception后，做必要处理 producerGroup 作用： 标识一类 Producer 可以通过运维工具查询这个发送消息应用下有多个 Producer 实例 发送分布式事务消息时，如果 Producer 中途意外宕机，Broker 会主动回调 Producer Group 内的任意一台机器来确认事务状态。 one producerGroup per application as default 一个 Producer Group 下包含多个 Producer 实例，可以是多台机器， 也可以是一台机器的多个进程，或者一个进程的多个 Producer 对象。 一个 Producer Group 可以发送多个 Topic 消息. 消费消息的默认约定：TODO 并行消息(多线程多Queue)有序消息(单线程单一ueue)延迟消息(单线程单一ueue)]]></content>
      <categories>
        <category>MQ</category>
        <category>RocketMQ</category>
      </categories>
      <tags>
        <tag>MQ</tag>
        <tag>RocketMQ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RocketMQ一基本概念]]></title>
    <url>%2FrocketMQ-1-concepts.html</url>
    <content type="text"><![CDATA[contents Main Concepts nameServer Broker producer consumer group 同步 - 异步 安装 单机 主从 控制台 mqadmin 控制台(rocketMq-console) Main ConceptsnameServer TODO broker TODO producer 生产者端的负载均衡（生产者发送时，会自动轮询当前所有可发送的broker，一条消息发送成功，下次换另外一个broker发送，以达到消息平均落到所有的broker上） 假如某个Broker宕机，意味生产者最长需要30秒才能感知到。在这期间会向宕机的Broker发送消息 ProducerGroupNameFor non-transactional messages, it does not matter as long as it’s unique per process. nameSrv]]></content>
      <categories>
        <category>MQ</category>
        <category>RocketMQ</category>
      </categories>
      <tags>
        <tag>MQ</tag>
        <tag>RocketMQ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[为什么需要消息中间件 以及 消息中间件的使用场景]]></title>
    <url>%2Fmq-why-and-where-to-use-it.html</url>
    <content type="text"><![CDATA[生产者消费者模式的思考 如何多机器部署运行 &lt;分布式&gt;？ 挑战：queue无法跨JVM共享 如何灾备以及负载均衡？ 挑战：需要一个controller做通知和分发数据 所以，需要一个独立部署的可以被多方访问的服务，其数据必须是多线程乃至分布式安全的 消息中间件是个好的选择 消息中间件的特征 queues （消息的容器） 发布/订阅（Publisher/Subscribe） （发布者发布1条数据，会被多个订阅者消费） 将增量式更新数据推给各个订阅的client，并保证每个client都收到 将数据平均发送给每个订阅者（负载均衡，例如：RocketMQ的CLUSTERING消息模型） P2P（Peer-to-Peer） （1条消息只会被消费一次） 对消息中间件的一般要求 服务的高可用性（主从或者分布式） 消息的低延迟、极少量丢失或不丢失 对消息中间件的进一步要求：消息不重复 支持事务 负载均衡 应用场景异步将传统线性程序，转为并发分布式 传统线性做法：1. 动作一 2. 动作二 利用消息中间件，并发处理。（发送一个订阅消息，然后由各个消费者并行处理） consumer1：动作一 （只需保证动作一最终一定成功便可，不关心时效性） consumer2：动作二 解耦在新增功能点时，无需更改主流程代码或逻辑 如果要在动作二后增加动作三，只需再增加相应的订阅者，无需更改主流程 consumer3：发送短信通知下单者支付成功。 把支付、退款从hotel移到到wfinance中统一管理 负载均衡 多个consumer分布在多个不同的机器上，热部署.如rocketMQ的集群模式 consumer.setMessageModel(MessageModel.CLUSTERING); 防治消息洪流 要求消息中间件有以下两个能力 强有力的消息堆积 持久化 消息队列可以作为 缓冲区域 存在 实例见下文的 日志收集 案例 push数据场景描述：各个client每10分钟请求一次我们的API，以获取我们的增量数据 日志收集 Kafka：接收用户日志的消息队列。Kafka可以快速接收所有收集过来的日志，并提供堆积和持久化功能，以便Logstash等后续消费者慢慢处理。 Logstash：做日志解析，统一成JSON输出给Elasticsearch。 Elasticsearch：实时日志分析服务的核心技术，一个schemaless，实时的数据存储服务，通过index组织数据，兼具强大的搜索和统计功能。 Kibana：基于Elasticsearch的数据可视化组件，超强的数据可视化能力是众多公司选择ELK stack的重要原因。 References : 系统间通信系列博文 RocketMQ简介及安装 消息队列的使用场景]]></content>
      <categories>
        <category>MQ</category>
      </categories>
      <tags>
        <tag>MQ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大规模分布式存储系统-原理解析与架构实战]]></title>
    <url>%2F%E5%A4%A7%E8%A7%84%E6%A8%A1%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%98%E5%82%A8%E7%B3%BB%E7%BB%9F-%E5%8E%9F%E7%90%86%E8%A7%A3%E6%9E%90%E4%B8%8E%E6%9E%B6%E6%9E%84%E5%AE%9E%E6%88%98.html</url>
    <content type="text"><![CDATA[第2章 单机存储系统 2.1 硬件基础 2.1.1 CPU架构 CMP &amp; NUMA 2.1.2 IO总线 主板的南北桥架构 2.1.3 网络拓扑 三层结构：接入层／汇聚层／核心层 三级CLOS网络（扁平化结构） 2.1.4 性能参数 2.1.5 存储层次架构 集群-&gt;机架-&gt;单机 存储系统的性能维度：吞吐量，访问延时。 2.2 单机存储引擎 2.2.1 哈希存储引擎 Bitcask -&gt; 基于哈希表结构的键值存储系统，仅支持追加操作；文件大小有限制 数据结构 定期合并 快速恢复 2.2.2 B树存储引擎 2.4 事务与并发控制 事务的并发通过锁来实现 提高读事务性能 写时复制（copy on write） 多版本并发控制 （Multi-version concurrency control) 2.4.1 事务 2.4.2 并发控制 数据库锁 发散课题 -&gt; read committed &amp; read committed snapshot &amp; repeated read &amp; MVCC的区别与实现 解决死锁的思路 为事务设置超时回滚 死锁检测，检测到死锁后回滚 写时复制，无锁 用SNAPSHOT代替锁 无锁，写不影响读 snapshot的生成成本高 写互斥写时复制B+树 多版本并发控制，无锁 两个隐藏列 行被修改时的版本 行被删除时的版本]]></content>
      <categories>
        <category>Storage</category>
      </categories>
      <tags>
        <tag>分布式</tag>
        <tag>存储</tag>
      </tags>
  </entry>
</search>
